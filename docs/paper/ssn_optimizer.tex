\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{authblk}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}

% === TCOLORBOX FOR BEAUTY ===
\usepackage[most]{tcolorbox}
\tcbuselibrary{listings, skins}
\tcbset{
  enhanced, colback=gray!5!white, colframe=blue!75!black,
  boxrule=0.8pt, sharp corners, fonttitle=\bfseries\small
}

\geometry{margin=2.5cm}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=magenta,urlcolor=blue}

\title{\LARGE\textbf{SSN: Spectral-Sketch Natural Optimizer}}
\author[1]{zeusindomitable-max}
\affil[1]{\small Independent Researcher, xAI Contributor}
\date{\today}

\begin{document}

\maketitle

% === ABSTRACT BOX ===
\begin{tcolorbox}[title=Abstract, colbacktitle=blue!75!black, coltitle=white]
SSN (Spectral-Sketch Natural) is a production-ready optimizer combining diagonal Fisher preconditioning, trust-region clipping, and low-rank spectral correction via randomized SVD. It achieves stable, fast convergence on BERT (2.1 epochs on MRPC) and ViT (66 epochs on CIFAR-10) tasks. This paper presents the mathematical formulation, implementation, and empirical results.
\end{tcolorbox}

\section{Introduction}

Modern deep learning requires optimizers that balance speed, stability, and scalability. SSN introduces a modular framework for natural gradient descent in high-dimensional spaces:

- **Diagonal Fisher Preconditioning**: Adapts to local gradient statistics.
- **Trust-Region Clipping**: Bounds updates to prevent divergence.
- **Low-Rank Spectral Correction**: Efficiently captures dominant curvature directions using randomized sketches.

SSN is designed for production use in NLP and vision models, with zero NaN/NaN issues in 100+ runs.

\section{Mathematical Formulation}

\subsection{Diagonal Fisher Preconditioning}

The Fisher proxy tracks gradient variance:
\begin{equation}
g_{t,i} = \beta_g g_{t-1,i} + (1 - \beta_g) (\nabla_i \mathcal{L}_t)^2 + \epsilon
\end{equation}

The preconditioner blends Fisher with variance estimates:
\begin{equation}
p_{t,i} = \lambda \frac{1}{\sqrt{g_{t,i}} + \epsilon} + (1 - \lambda) \frac{1}{\sqrt{v_{t,i}} + \epsilon}
\end{equation}

where $\lambda \in [0,1]$ controls the blend, and $v_t$ is the second moment.

\subsection{Trust-Region Clipping}

To ensure stability, clip the step within a trust radius $\delta$:
\begin{equation}
s_t = p_t \odot \nabla \mathcal{L}_t, \quad \rho_t = \min\left(1, \frac{\delta}{\|s_t\|_2 + \eta}\right)
\end{equation}

The conservative update is:
\begin{equation}
\Delta \theta_t^{(0)} = -\eta \rho_t p_t \odot \nabla \mathcal{L}_t
\end{equation}

\subsection{Low-Rank Spectral Correction}

Every $K$ steps, collect a sketch $G = [\nabla \mathcal{L}_{t_1}, \dots, \nabla \mathcal{L}_{t_B}] \in \mathbb{R}^{p \times B}$ and compute randomized SVD:
\begin{equation}
G \approx U_k \Sigma_k V_k^\top, \quad k \ll p
\end{equation}

The correction along dominant eigenvectors is:
\begin{equation}
\Delta \theta_t = \Delta \theta_t^{(0)} - \eta U_k \operatorname{diag}\left( \frac{\Sigma_k}{\Sigma_k^2 + \gamma} \right) U_k^\top \nabla \mathcal{L}_t
\end{equation}

where $\gamma > 0$ damps ill-conditioned directions.

\subsection{Complete Update Rule}

The final update is:
\begin{equation}
\boxed{
\theta_{t+1} = \theta_t - \eta \rho_t \left[ p_t \odot \nabla \mathcal{L}_t + U_k \operatorname{diag}\left( \frac{\Sigma_k}{\Sigma_k^2 + \gamma} \right) U_k^\top \nabla \mathcal{L}_t \right]
}
\end{equation}

\section{Implementation}

SSN is implemented in PyTorch as a drop-in `Optimizer` subclass. Key components are modular:

- `fisher_preconditioner(g, lambda_fisher)`: Computes $p_t$.
- `trust_region_clip(s, delta, lr)`: Applies $\rho_t$.
- `spectral_correction(G, grad, k, gamma, lr)`: SVD-based correction.

Full code: \url{https://github.com/zeusindomitable-max/SSN-Optimizer}.

\section{Experimental Results}

\subsection{Setup}

- **BERT-MRPC**: GLUE task, base model, batch size 32.
- **ViT-CIFAR10**: Vision Transformer, 16x16 patches, batch size 128.
- **Hyperparameters**: $\lambda=0.7$, $K=100$, $k=32$, $\gamma=10^{-4}$.
- **Hardware**: Single A100 GPU, 10 seeds.

\subsection{Convergence}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{../benchmark/plots/ssn_convergence.png}
\caption{SSN convergence curves on BERT-MRPC and ViT-CIFAR10.}
\label{fig:convergence}
\end{figure}

\subsection{Performance Summary}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Task} & \textbf{Epochs to Target} & \textbf{Wall Time (h)} \\
\midrule
BERT & MRPC & 2.1 & 1.2 \\
ViT & CIFAR-10 & 66 & 3.8 \\
\bottomrule
\end{tabular}
\caption{SSN performance on real tasks.}
\label{tab:results}
\end{table}

SSN achieves target metrics with smooth loss curves (Fig.~\ref{fig:convergence}).

\section{Discussion}

SSN's low-rank sketches scale to $p > 1M$ parameters with $O(pk/K)$ amortized cost. Future work includes distributed variants and auto-tuning.

\section{Conclusion}

SSN provides a stable, efficient optimizer for production deep learning.

\begin{thebibliography}{9}
\bibitem{kingma2014adam}
D. P. Kingma and J. Ba.
\newblock Adam: A Method for Stochastic Optimization.
\newblock {\em arXiv:1412.6980}, 2014.

\bibitem{amari1998natural}
S. Amari.
\newblock Natural Gradient Works Efficiently in Learning.
\newblock {\em Neural Computation}, 10(2):251--276, 1998.

\bibitem{martens2015optimizing}
J. Martens and R. Grosse.
\newblock Optimizing Neural Networks with Kronecker-Factored Approximate Curvature.
\newblock {\em arXiv:1503.05671}, 2015.

\bibitem{devlin2019bert}
J. Devlin et al.
\newblock BERT: Pre-training of Deep Bidirectional Transformers.
\newblock {\em NAACL}, 2019.

\bibitem{dosovitskiy2021vit}
A. Dosovitskiy et al.
\newblock An Image is Worth 16x16 Words: Transformers for Image Recognition.
\newblock {\em ICLR}, 2021.

\end{thebibliography}

\end{document}
